# Main FastAPI Application for Radar News Aggregator

from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional, List, Dict, Any
from datetime import datetime, timezone
from pydantic import BaseModel
import asyncio
import os
from collections import defaultdict
from enum import Enum
import json

import httpx

from src.services.aggregator import NewsAggregator
from src.services.news_collector import NewsCollectorService
from src.core.database import Database
from src.core.config import config
from src.core.task_queue import get_task_queue, TaskPriority
from src.core.anomaly_filter import get_anomaly_filter
from src.services.logging_service import get_logger
from src.models.news import News

# Logging
logger = get_logger(__name__)

# FastAPI App
app = FastAPI(
    title="Radar News Aggregator",
    description="API –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤",
    version="1.0.0"
)

# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize Database
Database.init_db()

# Global Instances
db = Database()
aggregator = NewsAggregator(db=db)
news_collector = NewsCollectorService(db=db)

# Task Queue
task_queue = get_task_queue(use_redis=False)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º in-memory –æ—á–µ—Ä–µ–¥—å

# Anomaly Filter
anomaly_filter = get_anomaly_filter()


# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Ollama
OLLAMA_HOST = os.getenv('OLLAMA_HOST', 'http://localhost:11434')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'gemma2:9b')
OLLAMA_TIMEOUT = float(os.getenv('OLLAMA_TIMEOUT', '60'))


class ArticleAnalysisStatus(str, Enum):
    PENDING = "pending"
    COMPLETED = "completed"
    FAILED = "failed"


# In-memory —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
analysis_state: Dict[str, Dict[str, Dict[str, Any]]] = defaultdict(dict)  # ticker -> article_id -> state
article_index: Dict[str, Dict[str, Any]] = {}  # article_id -> {"ticker": str, "state": {...}}
analysis_lock = asyncio.Lock()


# Task Handler –¥–ª—è —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
async def handle_collect_news_task(payload: dict):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–¥–∞—á–∏ —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    try:
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–¥–∞—á–∏ —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {payload.get('company_name')}")
        
        await news_collector.collect_news_with_cache(
            company_name=payload['company_name'],
            max_results_per_source=payload.get('max_results_per_source', 30),
            use_search=payload.get('use_search', True),
            use_social=payload.get('use_social', False),
            save_to_db=payload.get('save_to_db', True),
            start_date=datetime.fromisoformat(payload['start_date']) if payload.get('start_date') else None,
            end_date=datetime.fromisoformat(payload['end_date']) if payload.get('end_date') else None,
        )
        
        logger.info(f"–ó–∞–¥–∞—á–∞ —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {payload.get('company_name')}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}", exc_info=True)
        raise


# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∑–∞–¥–∞—á
task_queue.register_handler('collect_news', handle_collect_news_task)


async def handle_analyze_article_task(payload: dict):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∞–Ω–∞–ª–∏–∑–∞ –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏"""
    ticker = payload.get("ticker", "").upper()
    article_data = payload.get("article")

    if not ticker or not article_data:
        logger.error("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π payload –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–∏: %s", payload)
        return

    article = AnalyzeNewsArticle(**article_data)
    await update_analysis_state(ticker, article, ArticleAnalysisStatus.PENDING)

    try:
        logger.info(f"Starting analysis for article {article.id} ({ticker})")
        result = await call_ollama_for_article(article)
        await update_analysis_state(ticker, article, ArticleAnalysisStatus.COMPLETED, result=result)
        logger.info(f"Analysis completed for article {article.id} ({ticker})")
    except Exception as exc:
        error_message = str(exc)
        logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–∏ {article.id} ({ticker}): {error_message}", exc_info=True)
        await update_analysis_state(ticker, article, ArticleAnalysisStatus.FAILED, error=error_message)
        raise


task_queue.register_handler('analyze_news_article', handle_analyze_article_task)


@app.on_event("startup")
async def startup_event():
    """–ó–∞–ø—É—Å–∫ worker'–æ–≤ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    logger.info("–ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã –æ—á–µ—Ä–µ–¥–µ–π –∑–∞–¥–∞—á")
    await task_queue.start_workers(num_workers=3)  # 3 worker'–∞ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    logger.info("–°–∏—Å—Ç–µ–º–∞ –æ—á–µ—Ä–µ–¥–µ–π –∑–∞–¥–∞—á –∑–∞–ø—É—â–µ–Ω–∞")


@app.on_event("shutdown")
async def shutdown_event():
    """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ worker'–æ–≤ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
    logger.info("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–∏—Å—Ç–µ–º—ã –æ—á–µ—Ä–µ–¥–µ–π –∑–∞–¥–∞—á")
    await task_queue.stop_workers()
    logger.info("–°–∏—Å—Ç–µ–º–∞ –æ—á–µ—Ä–µ–¥–µ–π –∑–∞–¥–∞—á –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")


# Response Models
class NewsArticle(BaseModel):
    """Frontend compatible format"""
    id: str
    headline: str
    content: str
    source: str
    timestamp: int
    url: str
    sentiment: Optional[str] = None


class AnalyzeNewsArticle(BaseModel):
    id: str
    headline: str
    content: str
    source: str
    timestamp: int
    url: str


class QueueNewsAnalysisRequest(BaseModel):
    ticker: str
    articles: List[AnalyzeNewsArticle]
    force: bool = False


class ArticleAnalysisResult(BaseModel):
    article_id: str
    status: ArticleAnalysisStatus
    sentiment: Optional[str] = None
    summary: Optional[str] = None
    error: Optional[str] = None
    updated_at: datetime


# Adapter Functions
def news_item_to_article(news: News) -> NewsArticle:
    """Convert NewsItem to frontend NewsArticle format"""
    article_id = str(news.id) if news.id else str(hash(news.url))
    sentiment = None

    entry = article_index.get(article_id)
    if entry:
        state = entry.get("state")
        if state and state.get("status") == ArticleAnalysisStatus.COMPLETED:
            sentiment = state.get("sentiment")

    return NewsArticle(
        id=article_id,
        headline=news.title,
        content=news.content or "",
        source=news.source,
        timestamp=int(news.publish_date.timestamp() * 1000) if news.publish_date else int(news.collected_at.timestamp() * 1000),
        url=news.url,
        sentiment=sentiment,
    )


async def call_ollama_for_article(article: AnalyzeNewsArticle) -> Dict[str, str]:
    """–í—ã–∑–æ–≤ Ollama –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏"""
    prompt = (
        "–¢—ã –æ–ø—ã—Ç–Ω—ã–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â—É—é –Ω–æ–≤–æ—Å—Ç—å –∏ –æ—Ü–µ–Ω–∏ –µ—ë —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å.\n"
        "–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON —Å –∫–ª—é—á–∞–º–∏ 'sentiment' (Positive, Negative –∏–ª–∏ Neutral) –∏ 'summary' (–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ).\n"
        f"ID: {article.id}\n"
        f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.headline}\n"
        f"–¢–µ–∫—Å—Ç: {article.content[:1200]}"
    )

    payload = {
        "model": OLLAMA_MODEL,
        "format": "json",
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "stream": False,
        "keep_alive": "5m",
    }

    async with httpx.AsyncClient(timeout=OLLAMA_TIMEOUT) as client:
        response = await client.post(
            f"{OLLAMA_HOST}/api/chat",
            json=payload
        )
        if response.status_code != 200:
            body = response.text[:500]
            raise RuntimeError(f"Ollama request failed with status {response.status_code}: {body}")

        data = response.json()
        content = data.get("message", {}).get("content")
        if not content:
            raise RuntimeError("Empty response content from Ollama")

        try:
            parsed = json.loads(content)
        except json.JSONDecodeError as exc:
            raise RuntimeError(f"Failed to parse Ollama response: {exc}") from exc

        sentiment = parsed.get("sentiment")
        summary = parsed.get("summary")
        if not sentiment or not summary:
            raise RuntimeError("Ollama response missing required fields")

        normalized_sentiment = str(sentiment).strip().capitalize()
        if normalized_sentiment not in {"Positive", "Negative", "Neutral"}:
            normalized_sentiment = "Neutral"

        return {
            "sentiment": normalized_sentiment,
            "summary": str(summary).strip(),
        }


async def update_analysis_state(
    ticker: str,
    article: AnalyzeNewsArticle,
    status: ArticleAnalysisStatus,
    result: Optional[Dict[str, str]] = None,
    error: Optional[str] = None,
):
    ticker_key = ticker.upper()
    async with analysis_lock:
        state = analysis_state[ticker_key].get(article.id)
        if not state:
            state = {
                "article": article.model_dump(),
                "status": status,
                "sentiment": None,
                "summary": None,
                "error": None,
                "updated_at": datetime.now(timezone.utc),
            }
            analysis_state[ticker_key][article.id] = state
            article_index[article.id] = {"ticker": ticker_key, "state": state}

        state["status"] = status
        state["updated_at"] = datetime.now(timezone.utc)
        state["article"] = article.model_dump()
        if result:
            state["sentiment"] = result.get("sentiment")
            state["summary"] = result.get("summary")
            state["error"] = None
        if error:
            state["error"] = error


def state_to_result(article_id: str, state: Dict[str, Any]) -> ArticleAnalysisResult:
    status_value = state.get("status", ArticleAnalysisStatus.PENDING)
    if isinstance(status_value, ArticleAnalysisStatus):
        status = status_value
    else:
        status = ArticleAnalysisStatus(str(status_value))

    updated_at_value = state.get("updated_at")
    if isinstance(updated_at_value, datetime):
        updated_at = updated_at_value
    else:
        updated_at = datetime.now(timezone.utc)

    return ArticleAnalysisResult(
        article_id=article_id,
        status=status,
        sentiment=state.get("sentiment"),
        summary=state.get("summary"),
        error=state.get("error"),
        updated_at=updated_at,
    )


async def get_ticker_results_snapshot(ticker: str) -> List[ArticleAnalysisResult]:
    ticker_key = ticker.upper()
    async with analysis_lock:
        items = list(analysis_state.get(ticker_key, {}).items())
    return [state_to_result(article_id, state) for article_id, state in items]


# Request Models
class CollectNewsRequest(BaseModel):
    company_name: str
    max_results_per_source: int = 30
    use_search: bool = True
    use_social: bool = False
    save_to_db: bool = True
    start_date: Optional[str] = None
    end_date: Optional[str] = None


class AnomalyWebhook(BaseModel):
    """Webhook payload from Finam-Service"""
    ticker: str
    z_score: float
    delta: float
    direction: str  # "buy" or "sell"
    price: float
    timestamp: str
    timeframe: str  # "M1", "M5", "M30"


# Response Models (continued)
class NewsItem(BaseModel):
    company_name: str
    title: str
    content: str
    url: str
    source: str
    publish_date: datetime
    collected_at: datetime
    relevance_score: Optional[float] = None


class AggregateResponse(BaseModel):
    company_name: str
    total_results: int
    news: List[NewsItem]
    sources_used: dict


class StatsResponse(BaseModel):
    total_articles: int
    by_source: dict
    available_sources: dict


# Endpoints
@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Radar News Aggregator API",
        "version": "1.0.0",
        "docs": "/docs",
        "endpoints": {
            "collect": "/api/collect",
            "news": "/api/news",
            "stats": "/api/stats",
            "sources": "/api/sources"
        }
    }


@app.post("/api/collect", response_model=AggregateResponse)
async def collect_news(request: CollectNewsRequest):
    """
    –°–æ–±—Ä–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –æ –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    """
    logger.info(f"API –∑–∞–ø—Ä–æ—Å –Ω–∞ —Å–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è '{request.company_name}'")

    try:
        # Parse dates if provided
        start_date = datetime.fromisoformat(request.start_date.replace('Z', '+00:00')) if request.start_date else None
        end_date = datetime.fromisoformat(request.end_date.replace('Z', '+00:00')) if request.end_date else None

        news_items = await aggregator.collect_news(
            company_name=request.company_name,
            max_results_per_source=request.max_results_per_source,
            use_search=request.use_search,
            use_social=request.use_social,
            save_to_db=request.save_to_db,
            start_date=start_date,
            end_date=end_date
        )

        # –ü–æ–¥—Å—á–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        sources_count = {}
        for item in news_items:
            sources_count[item.source] = sources_count.get(item.source, 0) + 1

        return AggregateResponse(
            company_name=request.company_name,
            total_results=len(news_items),
            news=[NewsItem(**item.dict()) for item in news_items],
            sources_used=sources_count
        )

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}", exc_info=e)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {str(e)}")


@app.get("/api/news/{company_name}", response_model=List[NewsItem])
async def get_news(
    company_name: str,
    start_date: Optional[str] = Query(None, description="–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ –≤ ISO —Ñ–æ—Ä–º–∞—Ç–µ"),
    end_date: Optional[str] = Query(None, description="–î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –≤ ISO —Ñ–æ—Ä–º–∞—Ç–µ"),
    limit: int = Query(100, ge=1, le=500, description="–õ–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
):
    """
    –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    """
    logger.info(f"API –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è '{company_name}'")

    try:
        # Parse dates if provided
        start = datetime.fromisoformat(start_date.replace('Z', '+00:00')) if start_date else None
        end = datetime.fromisoformat(end_date.replace('Z', '+00:00')) if end_date else None

        # Get news with optional date filtering
        if start or end:
            news_items = db.get_news_by_company_and_period(company_name, start, end, limit)
        else:
            news_items = aggregator.get_news_from_db(company_name, limit)

        return [NewsItem(**item.dict()) for item in news_items]

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π: {e}", exc_info=e)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π: {str(e)}")


@app.get("/api/stats/{company_name}", response_model=StatsResponse)
async def get_stats(company_name: str):
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –Ω–æ–≤–æ—Å—Ç—è–º –∫–æ–º–ø–∞–Ω–∏–∏
    """
    logger.info(f"API –∑–∞–ø—Ä–æ—Å –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è '{company_name}'")

    try:
        stats = aggregator.get_stats(company_name)
        return StatsResponse(**stats)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}", exc_info=e)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {str(e)}")


@app.get("/api/sources")
async def get_available_sources():
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    """
    logger.info("API –∑–∞–ø—Ä–æ—Å –Ω–∞ —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")

    try:
        sources = aggregator.get_available_sources()
        return {
            "search_engines": sources['search'],
            "social_media": sources['social'],
            "total_available": sum(1 for v in {**sources['search'], **sources['social']}.values() if v)
        }

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}", exc_info=e)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {str(e)}")


@app.get("/health")
async def health_check():
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞
    """
    return {
        "status": "healthy",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "database": "connected" if db else "disconnected"
    }


@app.get("/api/ticker/{ticker}/company")
async def get_company_by_ticker(ticker: str):
    """
    –ü–æ–ª—É—á–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ —Ç–∏–∫–µ—Ä—É
    """
    logger.info(f"API –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è —Ç–∏–∫–µ—Ä–∞ '{ticker}'")

    try:
        company_name = db.get_company_by_ticker(ticker.upper())
        if not company_name:
            raise HTTPException(status_code=404, detail=f"–¢–∏–∫–µ—Ä '{ticker}' –Ω–µ –Ω–∞–π–¥–µ–Ω")

        return {
            "ticker": ticker.upper(),
            "company_name": company_name
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–∏: {e}", exc_info=e)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–∏: {str(e)}")


@app.get("/api/tickers")
async def get_all_tickers():
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤
    """
    logger.info("API –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤")

    try:
        tickers = db.get_all_tickers()
        return {
            "total": len(tickers),
            "tickers": tickers
        }

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–∏–∫–µ—Ä–æ–≤: {e}", exc_info=e)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–∏–∫–µ—Ä–æ–≤: {str(e)}")


@app.get("/api/news/ticker/{ticker}", response_model=List[NewsArticle])
async def get_news_by_ticker(
    ticker: str,
    start_date: Optional[str] = Query(None, description="–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ –≤ ISO —Ñ–æ—Ä–º–∞—Ç–µ"),
    end_date: Optional[str] = Query(None, description="–î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –≤ ISO —Ñ–æ—Ä–º–∞—Ç–µ"),
    limit: int = Query(100, ge=1, le=500, description="–õ–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
):
    """
    –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Ç–∏–∫–µ—Ä—É (frontend compatible format)
    """
    logger.info(f"API –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Ç–∏–∫–µ—Ä–∞ '{ticker}'")

    try:
        # Get company name by ticker
        company_name = db.get_company_by_ticker(ticker.upper())
        if not company_name:
            raise HTTPException(status_code=404, detail=f"–¢–∏–∫–µ—Ä '{ticker}' –Ω–µ –Ω–∞–π–¥–µ–Ω")

        # Parse dates if provided
        start = datetime.fromisoformat(start_date.replace('Z', '+00:00')) if start_date else None
        end = datetime.fromisoformat(end_date.replace('Z', '+00:00')) if end_date else None

        # Get news with optional date filtering
        if start or end:
            news_items = db.get_news_by_company_and_period(company_name, start, end, limit)
        else:
            news_items = aggregator.get_news_from_db(company_name, limit)

        # Convert to frontend format
        articles = [news_item_to_article(news) for news in news_items]
        return articles

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–∏–∫–µ—Ä—É: {e}", exc_info=e)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π: {str(e)}")


@app.post("/api/analyze/news")
async def queue_news_analysis(request: QueueNewsAnalysisRequest):
    ticker_key = request.ticker.upper()
    queued = 0
    skipped_completed = 0
    skipped_pending = 0

    for article in request.articles:
        article_data = article.model_dump()
        should_queue = False

        async with analysis_lock:
            state = analysis_state[ticker_key].get(article.id)
            if not state:
                state = {
                    "article": article_data,
                    "status": ArticleAnalysisStatus.PENDING,
                    "sentiment": None,
                    "summary": None,
                    "error": None,
                    "updated_at": datetime.now(timezone.utc),
                }
                analysis_state[ticker_key][article.id] = state
                article_index[article.id] = {"ticker": ticker_key, "state": state}
                should_queue = True
            else:
                state["article"] = article_data
                current_status = state.get("status", ArticleAnalysisStatus.PENDING)
                if not isinstance(current_status, ArticleAnalysisStatus):
                    current_status = ArticleAnalysisStatus(str(current_status))

                if request.force:
                    should_queue = True
                elif current_status == ArticleAnalysisStatus.COMPLETED:
                    skipped_completed += 1
                    should_queue = False
                elif current_status == ArticleAnalysisStatus.PENDING:
                    skipped_pending += 1
                    should_queue = False
                else:
                    should_queue = True

                if should_queue:
                    state["sentiment"] = None
                    state["summary"] = None
                    state["error"] = None
                    state["status"] = ArticleAnalysisStatus.PENDING
                    state["updated_at"] = datetime.now(timezone.utc)

        if should_queue:
            await task_queue.add_task(
                'analyze_news_article',
                payload={'ticker': ticker_key, 'article': article_data},
                priority=TaskPriority.HIGH,
                deduplicate=False
            )
            queued += 1

    results = await get_ticker_results_snapshot(ticker_key)
    pending_count = sum(1 for item in results if item.status == ArticleAnalysisStatus.PENDING)

    return {
        "ticker": ticker_key,
        "queued": queued,
        "skipped": {
            "completed": skipped_completed,
            "pending": skipped_pending,
        },
        "pending": pending_count,
        "results": [item.model_dump(mode="json") for item in results],
    }


@app.get("/api/analyze/news/{ticker}", response_model=List[ArticleAnalysisResult])
async def get_news_analysis_status(
    ticker: str,
    article_ids: Optional[List[str]] = Query(None, description="–°–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö ID –Ω–æ–≤–æ—Å—Ç–µ–π")
):
    ticker_key = ticker.upper()
    results = await get_ticker_results_snapshot(ticker_key)

    if article_ids:
        filter_ids = set(article_ids)
        results = [item for item in results if item.article_id in filter_ids]

    return results


@app.post("/api/webhook/anomaly")
async def handle_anomaly_webhook(anomaly: AnomalyWebhook):
    """
    Webhook –æ—Ç Finam-Service –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –∞–Ω–æ–º–∞–ª–∏–∏ z-score
    –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û: –£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è + –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—á–µ—Ä–µ–¥—å –∑–∞–¥–∞—á
    """
    logger.info(f"üîî Anomaly received: {anomaly.ticker} | Z-Score: {anomaly.z_score:.2f} | {anomaly.direction.upper()}")

    try:
        # –ü–æ–ª—É—á–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ —Ç–∏–∫–µ—Ä—É (—É–±—Ä–∞—Ç—å @MISX)
        ticker_clean = anomaly.ticker.replace('@MISX', '')
        company_name = db.get_company_by_ticker(ticker_clean)

        if not company_name:
            logger.warning(f"Ticker {anomaly.ticker} not found in database")
            return {"status": "skipped", "reason": "ticker_not_found", "ticker": anomaly.ticker}

        # –£–ú–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –û—Ü–µ–Ω–∫–∞ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –∞–Ω–æ–º–∞–ª–∏–∏
        anomaly_score = anomaly_filter.evaluate_anomaly(
            ticker=anomaly.ticker,
            z_score=anomaly.z_score,
            delta=anomaly.delta,
            price=anomaly.price,
            timestamp=anomaly.timestamp,
            timeframe=anomaly.timeframe
        )

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏
        if not anomaly_score.is_significant:
            logger.info(
                f"‚è≠Ô∏è  Anomaly skipped (low score): {anomaly.ticker} | "
                f"score={anomaly_score.score:.1f} | reasons={anomaly_score.reasons}"
            )
            return {
                "status": "filtered",
                "reason": "low_significance_score",
                "ticker": anomaly.ticker,
                "score": anomaly_score.score,
                "threshold": 50,
                "details": anomaly_score.to_dict()
            }

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∑–Ω–∞—á–∏–º—É—é –∞–Ω–æ–º–∞–ª–∏—é –≤ –ë–î
        anomaly_id = db.add_anomaly(
            ticker=anomaly.ticker,
            company_name=company_name,
            z_score=anomaly.z_score,
            delta=anomaly.delta,
            direction=anomaly.direction,
            price=anomaly.price,
            timestamp=anomaly.timestamp,
            timeframe=anomaly.timeframe,
            news_count=0  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –ø–æ—Å–ª–µ —Å–±–æ—Ä–∞
        )

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –æ—Ü–µ–Ω–∫–µ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
        if anomaly_score.score >= 80:
            priority = TaskPriority.HIGH
        elif anomaly_score.score >= 60:
            priority = TaskPriority.NORMAL
        else:
            priority = TaskPriority.LOW
        
        # –î–æ–±–∞–≤–∏—Ç—å –∑–∞–¥–∞—á—É —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –û–ß–ï–†–ï–î–¨
        task_id = await task_queue.add_task(
            task_type='collect_news',
            payload={
                'company_name': company_name,
                'max_results_per_source': 30,
                'use_search': True,
                'use_social': False,  # –û—Ç–∫–ª—é—á–µ–Ω–æ (Telegram flood wait)
                'save_to_db': True,
                'anomaly_id': anomaly_id,
            },
            priority=priority,
            deduplicate=True,
            dedup_window=300  # 5 –º–∏–Ω—É—Ç
        )

        logger.info(
            f"‚úÖ Significant anomaly queued: ID={anomaly_id}, task={task_id}, "
            f"priority={priority.name}, score={anomaly_score.score:.1f}"
        )

        return {
            "status": "queued",
            "anomaly_id": anomaly_id,
            "task_id": task_id,
            "ticker": anomaly.ticker,
            "company_name": company_name,
            "z_score": anomaly.z_score,
            "priority": priority.name,
            "direction": anomaly.direction,
            "significance_score": anomaly_score.score,
            "reasons": anomaly_score.reasons,
            "message": "Significant anomaly queued for processing"
        }

    except Exception as e:
        logger.error(f"Error processing anomaly: {e}", exc_info=e)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/quotes/{ticker}")
async def get_ticker_quotes(ticker: str):
    """
    –ü–æ–ª—É—á–∏—Ç—å –∫–æ—Ç–∏—Ä–æ–≤–∫–∏ –æ—Ç Finam-Service
    """
    logger.info(f"Fetching quotes for {ticker} from Finam-Service")

    try:
        import requests
        from datetime import timedelta

        # –î–æ–±–∞–≤–∏—Ç—å @MISX –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç (Finam —Ç—Ä–µ–±—É–µ—Ç —ç—Ç–æ—Ç —Ñ–æ—Ä–º–∞—Ç)
        ticker_finam = ticker.upper()
        if not ticker_finam.endswith('@MISX'):
            ticker_finam = f"{ticker_finam}@MISX"

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ Finam
        end_ts = int(datetime.now(timezone.utc).timestamp())

        response = requests.get(
            "http://localhost:8001/bars",
            params={"ticker_name": ticker_finam, "timestamp": end_ts - 60*60*2},  # 2 —á–∞—Å–∞ –Ω–∞–∑–∞–¥
            timeout=10
        )

        if response.ok:
            data = response.json()
            bars = data.get("bars", [])

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ StockPoint —Ñ–æ—Ä–º–∞—Ç –¥–ª—è frontend
            points = []
            base_time = datetime.now(timezone.utc) - timedelta(hours=2)

            for i, bar in enumerate(bars):
                point_time = base_time + timedelta(minutes=i)
                points.append({
                    "date": point_time.isoformat().split('T')[0],
                    "price": bar["close"]
                })

            return {"ticker": ticker, "quotes": points}
        else:
            raise HTTPException(status_code=502, detail="Finam service unavailable")

    except requests.exceptions.RequestException as e:
        logger.error(f"Error connecting to Finam: {e}")
        raise HTTPException(status_code=502, detail=f"Finam service error: {str(e)}")
    except Exception as e:
        logger.error(f"Error fetching quotes: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/anomalies/impactful")
async def get_impactful_anomalies(limit: int = Query(10, ge=1, le=50)):
    """
    –ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ø –∞–Ω–æ–º–∞–ª–∏–π —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏ (–≤–∞–∂–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –¥–ª—è –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
    """
    logger.info(f"Fetching top {limit} impactful anomalies")

    try:
        anomalies = db.get_impactful_anomalies(limit)

        # –î–ª—è –∫–∞–∂–¥–æ–π –∞–Ω–æ–º–∞–ª–∏–∏ –¥–æ–±–∞–≤–∏—Ç—å —Ç–æ–ø –Ω–æ–≤–æ—Å—Ç—å
        result = []
        for anomaly in anomalies:
            # –ü–æ–ª—É—á–∏—Ç—å –æ–¥–Ω—É —Ç–æ–ø-–Ω–æ–≤–æ—Å—Ç—å –¥–ª—è —ç—Ç–æ–π –∫–æ–º–ø–∞–Ω–∏–∏
            news_items = db.get_news_by_company(anomaly['company_name'], limit=1)

            top_news = None
            if news_items:
                news = news_items[0]
                top_news = {
                    "headline": news.title,
                    "url": news.url,
                    "source": news.source
                }

            result.append({
                "id": anomaly['id'],
                "ticker": anomaly['ticker'],
                "company_name": anomaly['company_name'],
                "z_score": anomaly['z_score'],
                "delta": anomaly['delta'],
                "direction": anomaly['direction'],
                "price": anomaly['price'],
                "timestamp": anomaly['timestamp'],
                "timeframe": anomaly['timeframe'],
                "news_count": anomaly['news_count'],
                "top_news": top_news
            })

        return {"anomalies": result, "total": len(result)}

    except Exception as e:
        logger.error(f"Error fetching impactful anomalies: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/queue/stats")
async def get_queue_stats():
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—á–µ—Ä–µ–¥–∏ –∑–∞–¥–∞—á
    """
    try:
        queue_stats = task_queue.get_stats()
        cache_stats = news_collector.get_cache_stats()
        
        return {
            "queue": queue_stats,
            "cache": cache_stats,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

    except Exception as e:
        logger.error(f"Error fetching queue stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/system/stats")
async def get_system_stats():
    """
    –ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏—Å—Ç–µ–º—ã (–æ—á–µ—Ä–µ–¥—å, –∫—ç—à, rate limits)
    """
    try:
        from src.core.rate_limiter import get_rate_limiters
        
        rate_limiters = get_rate_limiters()
        
        return {
            "queue": task_queue.get_stats(),
            "cache": news_collector.get_cache_stats(),
            "rate_limits": rate_limiters.get_all_stats(),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    except Exception as e:
        logger.error(f"Error fetching system stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/news/hot", response_model=List[NewsArticle])
async def get_hot_news(
    hours: int = Query(24, ge=1, le=168, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≥–æ—Ä—è—á–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"),
    limit: int = Query(50, ge=1, le=200, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π")
):
    """
    –ü–æ–ª—É—á–∏—Ç—å "–≥–æ—Ä—è—á–∏–µ –Ω–æ–≤–æ—Å—Ç–∏" - —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–æ –∑–Ω–∞—á–∏–º—ã–º–∏ –∞–Ω–æ–º–∞–ª–∏—è–º–∏ —Ä—ã–Ω–∫–∞
    
    –ì–æ—Ä—è—á–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ - —ç—Ç–æ –Ω–æ–≤–æ—Å—Ç–∏, –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞—Å–æ–≤ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ
    —Å –∫–æ–º–ø–∞–Ω–∏—è–º–∏, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –±—ã–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∑–Ω–∞—á–∏–º—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏ —Ü–µ–Ω (–≤—ã—Å–æ–∫–∏–π z-score).
    –ù–æ–≤–æ—Å—Ç–∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∞–Ω–æ–º–∞–ª–∏–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏.
    """
    logger.info(f"API –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ –≥–æ—Ä—è—á–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π (hours={hours}, limit={limit})")
    
    try:
        # –ü–æ–ª—É—á–∏—Ç—å –≥–æ—Ä—è—á–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –ë–î
        hot_news = db.get_hot_news(hours=hours, limit=limit)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è frontend
        articles = [news_item_to_article(news) for news in hot_news]
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(articles)} –≥–æ—Ä—è—á–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {hours} —á–∞—Å–æ–≤")
        
        return articles
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≥–æ—Ä—è—á–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≥–æ—Ä—è—á–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {str(e)}")


@app.get("/api/anomaly/ticker/{ticker}/stats")
async def get_ticker_anomaly_stats(ticker: str):
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∞–Ω–æ–º–∞–ª–∏–π –¥–ª—è —Ç–∏–∫–µ—Ä–∞
    """
    try:
        stats = anomaly_filter.get_ticker_stats(ticker.upper())
        return stats
    except Exception as e:
        logger.error(f"Error fetching ticker stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))


class ForecastRequest(BaseModel):
    ticker: str
    stock_data: List[Dict[str, Any]]  # [{date, price}, ...]
    analyzed_news: List[Dict[str, Any]]  # Analyzed news with sentiment


class ForecastResponse(BaseModel):
    forecast: List[Dict[str, Any]]  # [{date, price}, ...]
    analysis: str


async def call_ollama_for_forecast(
    ticker: str,
    stock_data: List[Dict[str, Any]],
    analyzed_news: List[Dict[str, Any]]
) -> ForecastResponse:
    """–í—ã–∑–æ–≤ Ollama –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞ —Ü–µ–Ω"""

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
    recent_prices = stock_data[-10:] if len(stock_data) > 10 else stock_data
    price_summary = ", ".join([f"{p['date']}: ${p['price']:.2f}" for p in recent_prices])

    # –ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π
    news_summary = []
    sentiment_counts = {"Positive": 0, "Negative": 0, "Neutral": 0}

    for news in analyzed_news[-5:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 –Ω–æ–≤–æ—Å—Ç–µ–π
        sentiment = news.get("sentiment", "Neutral")
        sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
        news_summary.append(f"- {news.get('headline', '')[:100]} (Sentiment: {sentiment})")

    news_text = "\n".join(news_summary) if news_summary else "–ù–µ—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"

    prompt = (
        f"–¢—ã –æ–ø—ã—Ç–Ω—ã–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ –ø–æ –∞–∫—Ü–∏—è–º {ticker} –∏ —Å–æ–∑–¥–∞–π 7-–¥–Ω–µ–≤–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ —Ü–µ–Ω.\n\n"
        f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Ü–µ–Ω—ã:\n{price_summary}\n\n"
        f"–ù–µ–¥–∞–≤–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:\n{news_text}\n\n"
        f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–µ–π: Positive={sentiment_counts['Positive']}, "
        f"Negative={sentiment_counts['Negative']}, Neutral={sentiment_counts['Neutral']}\n\n"
        "–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:\n"
        "{\n"
        '  "forecast": [{"date": "YYYY-MM-DD", "price": —á–∏—Å–ª–æ}, ...],  // 7 –¥–Ω–µ–π\n'
        '  "analysis": "–ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)"\n'
        "}\n"
    )

    payload = {
        "model": OLLAMA_MODEL,
        "format": "json",
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "stream": False,
        "keep_alive": "5m",
    }

    async with httpx.AsyncClient(timeout=OLLAMA_TIMEOUT) as client:
        response = await client.post(
            f"{OLLAMA_HOST}/api/chat",
            json=payload
        )
        if response.status_code != 200:
            body = response.text[:500]
            raise RuntimeError(f"Ollama forecast request failed with status {response.status_code}: {body}")

        data = response.json()
        content = data.get("message", {}).get("content")
        if not content:
            raise RuntimeError("Empty response content from Ollama")

        try:
            parsed = json.loads(content)
        except json.JSONDecodeError as exc:
            raise RuntimeError(f"Failed to parse Ollama forecast response: {exc}") from exc

        forecast = parsed.get("forecast")
        analysis = parsed.get("analysis")

        if not forecast or not analysis:
            raise RuntimeError("Ollama forecast response missing required fields")

        return ForecastResponse(
            forecast=forecast,
            analysis=str(analysis).strip()
        )


@app.post("/api/forecast/{ticker}", response_model=ForecastResponse)
async def generate_price_forecast(ticker: str, request: ForecastRequest):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ —Ü–µ–Ω –Ω–∞ –∞–∫—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º AI
    """
    logger.info(f"API –∑–∞–ø—Ä–æ—Å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è '{ticker}'")

    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if not request.stock_data or len(request.stock_data) < 5:
            raise HTTPException(
                status_code=400,
                detail="–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ (–º–∏–Ω–∏–º—É–º 5 —Ç–æ—á–µ–∫)"
            )

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ —á–µ—Ä–µ–∑ Ollama
        forecast_result = await call_ollama_for_forecast(
            ticker=ticker,
            stock_data=request.stock_data,
            analyzed_news=request.analyzed_news or []
        )

        logger.info(f"–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è '{ticker}' —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {len(forecast_result.forecast)} —Ç–æ—á–µ–∫")

        return forecast_result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è {ticker}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞: {str(e)}")


# Main Entry Point
if __name__ == "__main__":
    import uvicorn

    logger.info("–ó–∞–ø—É—Å–∫ Radar News Aggregator API...")

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info"
    )
